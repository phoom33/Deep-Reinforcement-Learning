{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import deque, namedtuple\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#set hyperparameters\n",
    "MINI_BATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 1000000\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 5000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "ACTION_REPEAT = 0\n",
    "UPDATE_FREQUENCY = 4\n",
    "LEARNING_RATE = 0.00025\n",
    "GRADIENT_MOMENTUM = 0.95\n",
    "SQUARED_GRADIENT_MOMENTUM = 0.95\n",
    "MIN_SQUARED_GRADIENT = 0.01\n",
    "INITIAL_EXPLORATION = 1\n",
    "FINAL_EXPLORATION = 0.1\n",
    "FINAL_EXPLORATION_FRAME = 1000\n",
    "REPLAY_START_SIZE = 5000\n",
    "NOOP_MAX = 30\n",
    "\n",
    "GAME = \"CartPole-v0\"\n",
    "NUM_EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#setup game env\n",
    "env = gym.envs.make(GAME)\n",
    "env.frameskip = ACTION_REPEAT\n",
    "NUMBER_OF_ACTIONS = env.action_space.n\n",
    "observation = env.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cliped_mean_squared_error(y_true, y_pred):\n",
    "    #return K.clip(K.mean(K.square(y_pred - y_true), axis=-1), -1, 1)\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    input_shape = (AGENT_HISTORY_LENGTH*observation.shape[0])\n",
    "    model.add(Dense(256,input_dim=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(NUMBER_OF_ACTIONS))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss=cliped_mean_squared_error, optimizer=RMSprop(lr=0.0001))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize everything\n",
    "episode_rewards = np.zeros(NUM_EPISODES)\n",
    "episode_lengths = np.zeros(NUM_EPISODES)\n",
    "loss = np.zeros(NUM_EPISODES)\n",
    "total_frame = 0\n",
    "all_frame = 0\n",
    "\n",
    "\n",
    "# replay memory\n",
    "replay_memory =  deque(maxlen = REPLAY_MEMORY_SIZE);\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "# state history\n",
    "state_history = deque(maxlen = AGENT_HISTORY_LENGTH);\n",
    "\n",
    "# The epsilon decay schedule\n",
    "epsilons = np.linspace(INITIAL_EXPLORATION, FINAL_EXPLORATION, FINAL_EXPLORATION_FRAME)\n",
    "\n",
    "# build model\n",
    "train_model = build_model()\n",
    "target_model = build_model()\n",
    "\n",
    "max_reward = 0\n",
    "max_ep = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i_episode in xrange(NUM_EPISODES):\n",
    "\n",
    "    state = env.reset()\n",
    "    # init state\n",
    "    state = np.array(state)\n",
    "    for _ in xrange(AGENT_HISTORY_LENGTH):\n",
    "        state_history.append(state)\n",
    "    state = np.array(state_history)\n",
    "    state = state.flatten()\n",
    "    \n",
    "    count_ran = 0\n",
    "    count_q = 0\n",
    " \n",
    "    \n",
    "    for t in itertools.count():\n",
    "        # step random action\n",
    "        if np.random.random() < epsilons[min(total_frame,FINAL_EXPLORATION_FRAME-1)]:\n",
    "                count_ran += 1\n",
    "                action = np.random.randint(NUMBER_OF_ACTIONS)       \n",
    "        else:\n",
    "                count_q += 1\n",
    "                q_values = train_model.predict(np.array([state]))[0]\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "       \n",
    "        if done == True :\n",
    "            reward = -5\n",
    "\n",
    "        # clip reward [-5,1]\n",
    "        reward = max(-5, min(1, reward))\n",
    "        # append next state\n",
    "\n",
    "        next_state = np.array(next_state)\n",
    "        state_history.append(next_state)\n",
    "        next_state = np.array(state_history)\n",
    "        next_state = next_state.flatten()\n",
    "        # Save transition to replay memory\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "        \n",
    "        # Update statistics\n",
    "        \n",
    "        all_frame += 1\n",
    "        if all_frame>REPLAY_START_SIZE :\n",
    "            total_frame += 1\n",
    "            episode_rewards[i_episode] += reward\n",
    "            episode_lengths[i_episode] = t\n",
    "        \n",
    "        # train network    \n",
    "        if total_frame % UPDATE_FREQUENCY == 0 and total_frame != 0 :\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, MINI_BATCH_SIZE)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values = train_model.predict(states_batch)\n",
    "            q_values_next = target_model.predict(next_states_batch)\n",
    "            new_q_values_batch = reward_batch + np.invert(done_batch).astype(np.float32) * DISCOUNT_FACTOR * np.amax(q_values_next, axis=1)\n",
    "            for b in xrange(MINI_BATCH_SIZE) :\n",
    "                q_values[b][action_batch[b]] = new_q_values_batch[b]\n",
    "            targets_batch = q_values\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            #print 'train'\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss[i_episode] = train_model.train_on_batch(states_batch, targets_batch)\n",
    "        \n",
    "        # check if terminated\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "                \n",
    "        # update target network    \n",
    "        if total_frame != 0 and total_frame % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            #print \"update\"\n",
    "            target_model.set_weights(train_model.get_weights())\n",
    "            # save model\n",
    "            #print 'save model'\n",
    "            save_name = 'dqn_carpole_{0}.h5'.format(total_frame)\n",
    "            #train_model.save(save_name)      \n",
    "            \n",
    "        if episode_rewards[i_episode] == 15000 :\n",
    "            train_model.save('dqn_carpole_high_score_{}.h5'.format(i_episode)) \n",
    "            \n",
    "    if max_reward < episode_rewards[i_episode] :\n",
    "        max_reward = episode_rewards[i_episode]\n",
    "        max_ep = i_episode\n",
    "        \n",
    "    print 'Done Episode:%i  reward:%i  random_action:%i  predict_action:%i' % (i_episode,episode_rewards[i_episode],count_ran, count_q)\n",
    "save_name = 'dqn_carpole_final_{0}.h5'.format(NUM_EPISODES)\n",
    "train_model.save(save_name)\n",
    "print max_ep\n",
    "print max_reward\n",
    "plt.plot(episode_rewards, lw=1)\n",
    "plt.show()\n",
    "plt.plot(loss, lw=1, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
