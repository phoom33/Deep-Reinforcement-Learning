{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque, namedtuple\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set hyperparameters\n",
    "MINI_BATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 1000000\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 10000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "ACTION_REPEAT = 4\n",
    "UPDATE_FREQUENCY = 4\n",
    "LEARNING_RATE = 0.00025\n",
    "GRADIENT_MOMENTUM = 0.95\n",
    "SQUARED_GRADIENT_MOMENTUM = 0.95\n",
    "MIN_SQUARED_GRADIENT = 0.01\n",
    "INITIAL_EXPLORATION = 1\n",
    "FINAL_EXPLORATION = 0.1\n",
    "FINAL_EXPLORATION_FRAME = 1000000\n",
    "REPLAY_START_SIZE = 50000\n",
    "NOOP_MAX = 30\n",
    "\n",
    "GAME = \"Breakout-v0\"\n",
    "FRAME_WIDTH = 150 \n",
    "FRAME_HEIGHT = 170 \n",
    "CROP_SIDE = 5\n",
    "CROP_TOP = 30\n",
    "CROP_BOTTOM = 10\n",
    "NUM_EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#setup game env\n",
    "env = gym.envs.make(GAME)\n",
    "env.frameskip = ACTION_REPEAT\n",
    "NUMBER_OF_ACTIONS = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def cliped_mean_squared_error(y_true, y_pred):\n",
    "    return K.clip(K.mean(K.square(y_pred - y_true), axis=-1), -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# process image\n",
    "def preprocess_state(state):\n",
    "    img = Image.fromarray(state)\n",
    "    img = img.crop(box=(CROP_SIDE, CROP_TOP, img.size[0]-CROP_SIDE, img.size[1]-CROP_BOTTOM))\n",
    "    img = img.resize((FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    img = img.convert('L') \n",
    "    return img;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "# input shape is  AGENT_HISTORY_LENGTH, FRAME_WIDTH, FRAME_HEIGHT\n",
    "# output shape is NUMBER_OF_ACTIONS\n",
    "# using 1 relu hidden layer \n",
    "# mean_squared_error as a loss function\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    input_shape = AGENT_HISTORY_LENGTH, FRAME_WIDTH, FRAME_HEIGHT\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        # (width, height, channels)\n",
    "        model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "    elif K.image_dim_ordering() == 'th':\n",
    "        # (channels, width, height)\n",
    "        model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "    model.add(Convolution2D(32, 8, 8, subsample=(4, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(NUMBER_OF_ACTIONS))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss=mean_squared_error, optimizer=RMSprop(lr=LEARNING_RATE))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize everything\n",
    "episode_rewards = np.zeros(NUM_EPISODES)\n",
    "episode_lengths = np.zeros(NUM_EPISODES)\n",
    "loss = np.zeros(NUM_EPISODES)\n",
    "total_frame = 0\n",
    "max_reward = 0\n",
    "max_ep = 0\n",
    "\n",
    "\n",
    "# replay memory\n",
    "replay_memory =  deque(maxlen = REPLAY_MEMORY_SIZE);\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "# state history\n",
    "state_history = deque(maxlen = AGENT_HISTORY_LENGTH);\n",
    "\n",
    "# The epsilon decay schedule\n",
    "epsilons = np.linspace(INITIAL_EXPLORATION, FINAL_EXPLORATION, FINAL_EXPLORATION_FRAME)\n",
    "\n",
    "# build model\n",
    "train_model = build_model()\n",
    "target_model = build_model()\n",
    "target_model.set_weights(train_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#init replay memmory\n",
    "\n",
    "replay_size = 0\n",
    "\n",
    "for i in itertools.count():\n",
    "\n",
    "    state = env.reset()\n",
    "    # init state\n",
    "    life = 0\n",
    "    state = preprocess_state(state)\n",
    "    state = np.array(state).astype('uint8')\n",
    "    for _ in xrange(AGENT_HISTORY_LENGTH):\n",
    "        state_history.append(state)\n",
    "    state = np.array(state_history)\n",
    "    \n",
    "\n",
    "    for t in itertools.count():\n",
    "        #  random action\n",
    "        action = np.random.randint(NUMBER_OF_ACTIONS)       \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # set the negative reward when lose lives  \n",
    "        if life > info['ale.lives'] :\n",
    "            reward = -5\n",
    "        life = info['ale.lives']\n",
    "        \n",
    "        # clip reward [-5,1]\n",
    "        reward = max(-5, min(1, reward))\n",
    "        \n",
    "        # append next state\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = np.array(next_state).astype('uint8')\n",
    "        state_history.append(next_state)\n",
    "        next_state = np.array(state_history)\n",
    "        \n",
    "        # Save transition to replay memory\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "        \n",
    "        # check if terminated\n",
    "        replay_size +=1\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "         \n",
    "    # stop when replay memory full    \n",
    "    if replay_size > REPLAY_START_SIZE :\n",
    "        print \"Done init replay memory (ep:%i)\" %(i)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i_episode in xrange(NUM_EPISODES):\n",
    "\n",
    "    state = env.reset()\n",
    "    # init state\n",
    "    state = preprocess_state(state)\n",
    "    state = np.array(state).astype('uint8')\n",
    "    for _ in xrange(AGENT_HISTORY_LENGTH):\n",
    "        state_history.append(state)\n",
    "    state = np.array(state_history)\n",
    "\n",
    "    count_ran = 0\n",
    "    count_q = 0\n",
    "    life = 0\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        # step random action\n",
    "        if np.random.random() < epsilons[min(total_frame,FINAL_EXPLORATION_FRAME-1)]:\n",
    "                count_ran += 1\n",
    "                action = np.random.randint(NUMBER_OF_ACTIONS)       \n",
    "        else:\n",
    "                count_q += 1\n",
    "                q_values = train_model.predict(np.array([state]))[0]\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # set the negative reward when lose lives  \n",
    "        if life > info['ale.lives'] :\n",
    "            reward = -5\n",
    "        life = info['ale.lives']\n",
    "        \n",
    "        # clip reward [-5,1]\n",
    "        reward = max(-5, min(1, reward))\n",
    "        \n",
    "        # append next state\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = np.array(next_state).astype('uint8')\n",
    "        state_history.append(next_state)\n",
    "        next_state = np.array(state_history)\n",
    "        \n",
    "        # Save transition to replay memory\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "        \n",
    "        # Update statistics\n",
    "        total_frame += 1\n",
    "        episode_rewards[i_episode] += reward\n",
    "        episode_lengths[i_episode] = t\n",
    "        \n",
    "        # train network    \n",
    "        if total_frame % UPDATE_FREQUENCY == 0 and total_frame != 0 :\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, MINI_BATCH_SIZE)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values = train_model.predict(states_batch)\n",
    "            q_values_next = target_model.predict(next_states_batch)\n",
    "            new_q_values_batch = reward_batch + np.invert(done_batch).astype(np.float32) * DISCOUNT_FACTOR * np.amax(q_values_next, axis=1)\n",
    "            for b in xrange(MINI_BATCH_SIZE) :\n",
    "                q_values[b][action_batch[b]] = new_q_values_batch[b]\n",
    "            targets_batch = q_values\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss[i_episode] = train_model.train_on_batch(states_batch, targets_batch)\n",
    "        \n",
    "\n",
    "        # check if terminated\n",
    "        if done :\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "                \n",
    "        # update target network    \n",
    "        if total_frame != 0 and total_frame % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            target_model.set_weights(train_model.get_weights())      \n",
    "    \n",
    "    ####### EPISODE END\n",
    "    \n",
    "        \n",
    "    if max_reward < episode_rewards[i_episode] :\n",
    "        max_reward = episode_rewards[i_episode]\n",
    "        max_ep = i_episode\n",
    "    # print statistics    \n",
    "    print 'Ep:%i\\treward:%i\\trandom_act:%i\\tpredict_act:%i' % (i_episode,episode_rewards[i_episode],count_ran, count_q)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print statistics  \n",
    "print \"max score:{} at episode:{}\".format(max_reward,max_ep)\n",
    "save_name = 'dqn.h5'\n",
    "train_model.save(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
